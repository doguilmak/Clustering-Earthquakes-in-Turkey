# -*- coding: utf-8 -*-
"""earthquake_afad.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_9HFvibKfXAtge5BgKXRclb1adEOdTdM

<h1 align=center><font size = 5>Clustering Earthquakes in Turkey with DBSCAN</font></h1>

<h4 align=center>From 6th February to 27th April</h4>

<br>

<img src="https://pbs.twimg.com/media/Fo2gx2kX0AItpFs?format=jpg&name=large" width=1000 height=500 alt="https://github.com/doguilmak/Clustering-Earthquakes-in-Turkey">

<small>Picture Source: <a href="https://www.esa.int/ESA_Multimedia/Images/2023/02/Horizontal_offsets_from_Sentinel-2">European Space Agency</a>

<br>

<h2>Keywords</h2>
<ul>
  <li>Geology</li>
  <li>Earth Science</li>
  <li>Earthquake</li>
  <li>Turkey</li>
  <li>DBSCAN</li>
</ul>

<br>

<h2>Definition of Earthquake</h2>

<p>An earthquake is the shaking of the surface of the Earth resulting from a sudden release of energy in the Earth's <i>lithosphere</i> that creates <i>seismic waves</i>. People can scale <i>seismic waves</i> as <i>The Richter scale</i>.</p>

<br>

<h2>Definition of the Richter Scale</h2>

<p>The Richter scale —also called the Richter magnitude scale, Richter's magnitude scale, and the <i>Gutenberg–Richter</i> scale—is a measure of the strength of earthquakes, developed by <i>Charles Francis Richter</i> and presented in his landmark <i>1935</i> paper, where he called it the "magnitude scale".This was later revised and renamed the local magnitude scale, denoted as $ML$ or $M_{L}$.</p>

<br>

$$M_{L} = log_{10} A - log_{10} A_{0}(δ) = log_{10} [A/A_{0}(δ)]$$

<br>

<p>$A$ is the maximum excursion of the Wood–Anderson seismograph</p>

<p>The empirical function $A_{0}$ depends only on the epicentral distance of the station, $δ$. In practice, readings from all observing stations are averaged after adjustment with station-specific corrections to obtain the $M_{L}$ value.</p>

<br>

<h2>AFAD Event Catalog</h2>

<p>The AFAD Event Catalog is a database of earthquakes that have occurred in and around Turkey. AFAD stands for the Disaster and Emergency Management Presidency of Turkey (in Turkish: Afet ve Acil Durum Yönetimi Başkanlığı), which is responsible for managing <i>disaster</i> and <i>emergency situations</i> in the country.

The AFAD Event Catalog provides information about <i>earthquakes</i> that have occurred in Turkey, as well as earthquakes in neighboring countries that have been felt in Turkey. The catalog includes data on the location, magnitude, depth, and time of the <i>earthquakes</i>, as well as other details such as the number of <i>casualties</i> and the extent of damage.

The AFAD Event Catalog is an important tool for <i>earthquake</i> research and preparedness in Turkey, as it allows scientists and emergency managers to better understand the patterns and characteristics of <i>earthquakes</i> in the region. It is also a valuable resource for the public, as it provides up-to-date information about earthquakes that can help people stay informed and take appropriate safety measures.</p>

<br>

<h3>Data Link</h3>

You can take a look at original website of <a href='https://deprem.afad.gov.tr/event-catalog'>AFAD Event Catalog.</a>

<br>

<h2>License</h2>

<p>MIT License</p>

<br>

<h3>Sources</h3>
<ul>
    <li><a href="https://en.wikipedia.org/wiki/Richter_magnitude_scale">Wikipedia</a></li>
    <li><a href="https://deprem.afad.gov.tr/event-catalog">AFAD Event Catalog</a></li>
</ul>

<br>

<h2>Table of Contents</h2>

<p>The <i>magnitude</i> of the <i>earthquakes</i> has been visualized on the plot and clustered by <i>DBSCAN</i> in Turkey.</p>

<div class="alert alert-block alert-info" style="margin-top: 20px">
<li><a href="https://#import">Import Libraries for Model</a></li>
<li><a href="https://#data_preparation">Dataset Preparation (Data Preprocessing)</a></li>
<li><a href="https://#dbscan">Clustering with DBSCAN</a></li>

<br>

<p>Estimated Time Needed: <strong>20 min</strong></p>

</div>

<br>
<h2 align=center id="import">Import Libraries for Model</h2>
<p>The following are the libraries we are going to use for this lab:</p>
"""

!pip3 install basemap -q

!pip3 install basemap-data-hires -q

!pip3 install matplotlib -q

from datetime import date
from datetime import timedelta
import datetime

import pandas as pd
import numpy as np

from sklearn.cluster import DBSCAN 
import sklearn.utils
from sklearn.preprocessing import StandardScaler

# Commented out IPython magic to ensure Python compatibility.
from mpl_toolkits.basemap import Basemap

import matplotlib as mpl
import matplotlib
import matplotlib.pyplot as plt
matplotlib.__version__

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/

# Commented out IPython magic to ensure Python compatibility.
# %ls

"""<br>
<h2 align=center id="data_preparation">Dataset Preparation (Data Preprocessing)</h2>

<p>You should do following steps for the dataset. First, you need to <a href='https://deprem.afad.gov.tr/event-catalog'>AFAD Event Catalog</a> website in you want to dowload your own dataset. After that, you should select date range (Tarih Aralığı(UTC)). In addition, you can do more specific studies by entering parameters such as latitude range, longitude range, depth range, size range and location.</p>

<br>

<img src="https://raw.githubusercontent.com/doguilmak/Clustering-Earthquakes-in-Turkey/main/assets/AFAD_event_catalog_step_1.png" width=1000 height=500 alt="https://github.com/doguilmak/Clustering-Earthquakes-in-Turkey">

<br>

<p>Then, you shold click on Filtrele button indicated by the blue arrow. After that, you can see all the earthquakes happened in spesific date range. Now, we can download the data. For that, all we need to do is click on CSV button on the bottom.</p>

<br>

<p>Loading dataset.</p>
"""

path = '/content/data.csv'

df = pd.read_csv(path)

df.head()

"""<p>We can modify our <i>Date</i> column. We can separate the time and date values ​​as separate columns. After that, we don't need <i>Date</i> column anymore.</p>"""

df[['Dates', 'Time']] = df['Date'].str.split('T', 1, expand=True)
df.drop(columns=['Date'], inplace=True)

df.head()

"""<p>Now, we can convert <i>Dates</i> and <i>Time</i> columns into date and time type.</p>"""

df['Dates'] = pd.to_datetime(df['Dates'])

df['Time'] = pd.to_datetime(df['Time'])
df['Time'] = [time.time() for time in df['Time']]

"""<p>In addition, I wanted to change <i>Location</i> column too. Let's seperate them into <i>Town</i> and <i>City</i> columns."""

df[['Town', 'City']] = df['Location'].str.split(' ', 1, expand=True)
df.drop(columns=['Location'], inplace=True)

"""<p>We should remove parentheses from <i>City</i> column.</p>"""

df['City'] = df['City'].apply(lambda x: x.replace('(', '').replace(')', ''))

df.head()

"""<p>Let's look for anomalies and missing values.</p>"""

df.info()

print("Number of NaN values: {}.".format(df.isnull().sum().sum()))

print("Number of duplicated rows: {}.".format(df.duplicated().sum()))

"""<h3>Filter Data</h3>

<p>Filtering data with pandas is a way to extract a subset of a dataframe that meets certain criteria or conditions. The purpose of filtering is to focus on a specific subset of the data that is relevant to a particular analysis or question.

In pandas, filtering can be done using conditional statements, which allow you to specify the conditions that rows must meet to be included in the filtered dataframe.</p>

<p>We can filter the data according to the magnitude of the earthquake.</p>
"""

max(df['Magnitude'])

df[df['Magnitude'] > 7]

"""<p>We can filter the data according to the date of the earthquake.</p>"""

df[df['Dates'] == '2023-02-06'].head()

"""<p>We can filter the data according to the city.</p>"""

df[df['City'] == 'Kahramanmaraş'].head()

"""<p>We can filter the data according to the town.</p>"""

df[df['Town'] == 'Göksun'].head()

"""<p>We can filter the data according to the date and city.</p>"""

df[(df['City'] == 'Kahramanmaraş') & (df['Dates'] == '2023-02-06')].head()

"""<p>We can filter the data according to the magnitude, date and city.</p>"""

df[(df['Magnitude'] > 4) & (df['City'] == 'Kahramanmaraş') & (df['Dates'] == '2023-02-06')].head()

"""<h3>Statistical Inferences</h3>

<h4>Histogram</h4>

<p>A magnitude histogram plot can help us to understand the range and frequency of magnitude values in a dataset, which can be useful for identifying patterns or trends in the data.</p>
"""

fig, ax = plt.subplots(figsize=(12, 5))

n, bins, patches = ax.hist(df['Magnitude'], bins=15, edgecolor='white', alpha=0.8, color='#1f77b4')
ax.set_xlabel('Magnitude', fontsize=14)
ax.set_ylabel('Frequency', fontsize=14)
ax.tick_params(axis='both', which='major', labelsize=12)

ax.set_title('Distribution of Magnitude', fontsize=16)
ax.grid(axis='y', alpha=0.5)

for patch in patches:
    patch.set_linewidth(1)
    patch.set_edgecolor('white')
    
ax.legend(['Magnitudes'], fontsize=12)
plt.show()

"""<p>The <i>x-axis</i> of the plot represents the <i>magnitude</i> values, while the <i>y-axis</i> represents the <i>frequency</i> of occurrence of each <i>magnitude</i> value. The bars in the plot represent the <i>frequency of magnitude</i> values within each bin, which can be customized to control the number of bins and the width of each bin. By examining the shape and distribution of the <i>magnitude histogram plot</i>, we can gain insights into the characteristics of the data, such as the presence of outliers, the overall level of variability, and the central tendency of the magnitude values.</p>

<h4>Box Plot and Iterquartile Range</h4>

A box plot is a graphical representation of the distribution of data that displays the median, quartiles, and outliers. The box plot for the Magnitude column shows the following features:

<ul>
  <li>The box represents the interquartile range (IQR), which is the range of the middle 50% of the data.</li>
  <li>The bottom and top of the box indicate the 25th and 75th percentiles, respectively.</li>
  <li>The line inside the box represents the median, which is the middle value of the data.</li>
  <li>The whiskers extend from the box to the smallest and largest observations that are not outliers.</li>
  <li>Outliers are plotted as individual points outside the whiskers.</li>
  <li>The notch in the box represents the confidence interval of the median. If the notches of two box plots do not overlap, it suggests that there is a significant difference between the medians of the two groups.</li>
</ul>
"""

fig, ax = plt.subplots(figsize=(12, 5))

plt.boxplot(df['Magnitude'], notch=True, vert=False, showmeans=True, meanline=True, 
            patch_artist=True, boxprops=dict(facecolor='lightblue', color='blue'),
            medianprops=dict(color='red'), whiskerprops=dict(color='gray'),
            capprops=dict(color='black'), flierprops=dict(marker='o', markerfacecolor='green', markersize=8))

plt.title("Magnitude Box Plot", fontsize=16)
plt.xlabel("Magnitude", fontsize=12)
plt.ylabel("", fontsize=12)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.show()

"""<p>For the beggining, let's calculate presence of outliers from <i>Magnitude</i> column. From the beggining, we should calculate the <i>Iterquartile Range (IQR)</i> of the <i>Magnitude</i> column.</p>"""

Q1 = df['Magnitude'].quantile(0.25)
Q3 = df['Magnitude'].quantile(0.75)
IQR = Q3 - Q1

"""<p>Now, we can identify potential outliers as values that are more than 1.5 times the <i>IQR</i> away from the <i>median</i>.</p>"""

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
outliers = df[(df['Magnitude'] < lower_bound) | (df['Magnitude'] > upper_bound)]

print("Number of potential outliers:", len(outliers))

print("Potential outlier values: ")
outliers['Magnitude'].head(10)

"""<p>Overall, the box plot for the Magnitude column provides a visual summary of the central tendency, spread, and outliers of the data. It can be used to compare the Magnitude data to other datasets or to identify unusual values in the data.</p>

<h4>Standard Deviation, Mean and Median</h4>

<p>Let's calculate the overall level of variability from the Magnitude column using a measure called the <i>standard deviation</i>. The <i>standard deviation</i> measures how much the values in a dataset deviate from the mean value. A higher <i>standard deviation</i> indicates greater variability in the data.</p>
"""

magnitude_std = df['Magnitude'].std()

print("The standard deviation of the Magnitude column is: ", magnitude_std)

"""<p>In addition, let's calculate the <i>central tendency</i> of the <i>Magnitude</i> column using measures like the <i>mean</i> and <i>median</i>. These measures can provide information about the typical or representative value of the <i>Magnitude</i> data.</p>"""

magnitude_mean = df['Magnitude'].mean()
magnitude_median = df['Magnitude'].median()

print("The mean of the Magnitude column is:", magnitude_mean)
print("The median of the Magnitude column is:", magnitude_median)

"""<br>
<h2 align=center id="dbscan">Clustering with DBSCAN and Visualization</h2>

<p><i>DBSCAN (Density-Based Spatial Clustering of Applications with Noise)</i> is a <i>clustering algorithm</i> used in <i>machine learning</i> and <i>data mining</i>. It is a <i>density-based clustering algorithm</i>, which means that it groups together data points that are close to each other in <i>space</i> and <i>density</i>, and separates them from other <i>regions</i> of <i>lower density</i>. The algorithm is particularly effective at identifying <i>clusters of arbitrary shape</i>, as well as clusters that contain <i>noise</i> or <i>outliers</i>.</p>

<p><i>The DBSCAN algorithm</i> is useful in many applications, including <i>image processing</i>, <i>natural language processing</i>, and <i>anomaly detection</i>. It is especially powerful in cases where the data has complex structure and traditional clustering algorithms like <i>K-means</i> may not work well. However, the performance of the algorithm can be sensitive to the choice of parameters, and it can be computationally expensive for large datasets.</p>

<p>The matplotlib basemap toolkit is a library for plotting 2D data on maps in Python. Basemap does not do any plotting on it’s own, but provides the facilities to transform coordinates to a map projections.</p>
"""

date = datetime.datetime.utcnow()

mpl.rcParams['figure.figsize'] = (20, 10)

"""<p>Through our data, we should define edges of the earthquakes.</p>"""

max_lat = df['Latitude'].max()
max_lat = max_lat + 1
min_lat = df['Latitude'].min()
min_lat = min_lat - 1

max_lon = df['Longitude'].max()
max_lon = max_lon + 1
min_lon = df['Longitude'].min()
min_lon = min_lon - 1

my_map = Basemap(projection='merc',
            resolution = 'h', area_thresh = 100.0,
            llcrnrlon=min_lon, llcrnrlat=min_lat, #min longitude (llcrnrlon) and latitude (llcrnrlat)
            urcrnrlon=max_lon, urcrnrlat=max_lat) #max longitude (urcrnrlon) and latitude (urcrnrlat)

my_map.drawcoastlines()
my_map.drawcountries()
my_map.fillcontinents(color = 'white', alpha = 0.3)
my_map.shadedrelief()
my_map.nightshade(date)
parallels = np.arange(round(min_lat), round(max_lat), 1)
my_map.drawparallels(parallels,labels=[True,True,True,True], color='#A9A9A9')
meridians = np.arange(round(min_lon), round(max_lon), 1)
my_map.drawmeridians(meridians,labels=[True,True,True,True], color='#A9A9A9')

xs, ys = my_map(np.asarray(df.Longitude), np.asarray(df.Latitude))
df['xm'] = xs.tolist()
df['ym'] =ys.tolist()

"""<p>The density of <i>earthquakes</i> based on location refers to the <i>distribution</i> of <i>earthquake</i> occurrences across a geographical area. In general, areas with a high density of earthquakes are considered to be more <i>seismically active</i> than areas with a <i>low density</i> of <i>earthquakes</i>.

<i>Earthquake density</i> can be represented using various graphical methods, such as <i>heat maps</i> or <i>contour maps</i>. These maps typically use color or shading to indicate the <i>frequency or intensity of earthquakes</i> in different parts of the region of interest. For example, areas with <i>high earthquake density</i> might be colored in red, while <i>areas with low density</i> might be colored in blue or green.

Understanding <i>earthquake density</i> is <b>important for assessing seismic risk and designing earthquake-resistant structures. It can also help scientists better understand the underlying causes of earthquakes and develop more accurate earthquake prediction models.</b></p>
"""

lon = df['Longitude']
lat = df['Latitude']
mag = df['Magnitude']

my_map.shadedrelief()
my_map.drawcoastlines(color='gray')
my_map.drawcountries(color='gray')
my_map.fillcontinents(color = 'white', alpha = 0.1)
my_map.nightshade(date)
parallels = np.arange(round(min_lat), round(max_lat), 1)
my_map.drawparallels(parallels,labels=[True,True,True,True], color='#A9A9A9')
meridians = np.arange(round(min_lon), round(max_lon), 1)
my_map.drawmeridians(meridians,labels=[True,True,True,True], color='#A9A9A9')

nx, ny = 40, 40
lon_bins = np.linspace(min_lon, max_lon, nx+1)
lat_bins = np.linspace(min_lat, max_lat, ny+1)

density, _, _ = np.histogram2d(lon, lat, [lon_bins, lat_bins])

a = my_map.imshow(density.T, interpolation='spline36', alpha=0.7, cmap='YlOrBr', vmin=0, vmax=34)

for index, row in df.iterrows():
   my_map.plot(row.xm, row.ym, markerfacecolor =([0.7, 0, 0]),  marker='o', markersize= 5, alpha = 0.75)
plt.show()

my_map.shadedrelief()
my_map.drawcoastlines(color='gray')
my_map.drawcountries(color='gray')
my_map.fillcontinents(color = 'white', alpha = 0.1)
my_map.nightshade(date)
parallels = np.arange(round(min_lat), round(max_lat), 1)
my_map.drawparallels(parallels,labels=[True,True,True,True], color='#A9A9A9')
meridians = np.arange(round(min_lon), round(max_lon), 1)
my_map.drawmeridians(meridians,labels=[True,True,True,True], color='#A9A9A9')

for index, row in df.iterrows():
   my_map.plot(row.xm, row.ym, markerfacecolor =([1, 0, 0]),  marker='o', markersize= 5, alpha = 0.75)
plt.show()

"""<h3>Clustering of Stations Based on Their Magnitude</h3>

<p><i>DBSCAN</i> form sklearn library can run <i>DBSCAN</i> clustering from vector array or distance matrix. In our case, we pass it the <i>NumPy</i> array <code>Clus_dataSet</code> to find core samples of <i>high density</i> and expands <i>clusters</i> from them.</p>
"""

sklearn.utils.check_random_state(100)
Clus_dataSet = df[['xm', 'ym']]
Clus_dataSet = np.nan_to_num(Clus_dataSet)
Clus_dataSet = StandardScaler().fit_transform(Clus_dataSet)

"""<p>Computing DBSCAN.</p>"""

df.head()

"""<br>

<p><i>The DBSCAN algorithm</i> works by defining two parameters: <code>epsilon</code> (ε) and <code>min_samples</code>. <i>Epsilon</i> is a distance threshold that defines the maximum distance between two points for them to be considered part of the same cluster. <i>Min_samples</i> is the minimum number of points required to form a cluster. The algorithm then proceeds as follows:</p>

<ol>
  <li>Randomly select a point from the dataset that has not been visited.</li>
  <li>Find all the points within epsilon distance of this point, forming a "neighborhood".</li>
  <li>If the neighborhood contains fewer than min_samples points, label the point as noise.</li>
  <li>Otherwise, label the point and all points in the neighborhood as being part of the same cluster.</li>
  <li>Repeat steps 1-4 until all points have been visited.</li>
<ol>
"""

db = DBSCAN(eps=0.15, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
df["Clus_Db"]=labels

labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels))

df[["Magnitude", "Clus_Db"]].head()

set(labels)

"""<h4>Visualization of Clusters Based on Location</h4>"""

my_map.shadedrelief()
my_map.drawcoastlines(color='gray')
my_map.drawcountries(color='gray')
my_map.fillcontinents(color = 'white', alpha = 0.1)
my_map.nightshade(date)
parallels = np.arange(round(min_lat), round(max_lat), 1)
my_map.drawparallels(parallels,labels=[True,True,True,True], color='#A9A9A9')
meridians = np.arange(round(min_lon), round(max_lon), 1)
my_map.drawmeridians(meridians,labels=[True,True,True,True], color='#A9A9A9')

colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))

for clust_number in set(labels):
    c=(([0.4, 0.4, 0.4]) if clust_number == -1 else colors[np.int(clust_number)])
    clust_set = df[df.Clus_Db == clust_number]                    
    my_map.scatter(clust_set.xm, clust_set.ym, color=c,  marker='o', s=10, alpha = 0.85)
    if clust_number != -1:
        cenx=np.mean(clust_set.xm) 
        ceny=np.mean(clust_set.ym) 
        plt.text(cenx, ceny, str(clust_number), fontsize=18, color='red')
        print ("Cluster " + str(clust_number)+', Avg Magnitude: '+ str(np.mean(clust_set.Magnitude)))

"""<p>In addition, we can save our modified CSV file. <code>to_csv()</code> function is used to save the DataFrame to a CSV file. The first argument is the filename you want to use for the output file</p>"""

df.to_csv('modified_data.csv', index=False, encoding='utf-8-sig')

"""<br>

<h2>Contact Me</h2>
<p>If you have something to say to me please contact me:</p>

<ul>
  <li>Twitter: <a href="https://twitter.com/Doguilmak">Doguilmak</a></li>
  <li>Mail address: doguilmak@gmail.com</li>
</ul>
"""

from datetime import datetime
print(f"Changes have been made to the project on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")