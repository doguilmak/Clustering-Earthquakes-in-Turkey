# -*- coding: utf-8 -*-
"""earthquake_boun_month.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GxYTsi_aTWdUrYSvkXSgep8f1TPf_MYh

<h1 align=center><font size = 5>Clustering Earthquakes in Turkey with DBSCAN</font></h1>

<h4 align=center>Earthquakes within a Month (February)</h4>

<br>

<img src="https://raw.githubusercontent.com/doguilmak/Clustering-Earthquakes-in-Turkey/main/assets/boun_earthquake.jpg" width=1000 height=500 alt="https://github.com/doguilmak/Clustering-Earthquakes-in-Turkey">

<small>Picture Source: <a href="https://github.com/doguilmak/Clustering-Earthquakes-in-Turkey">Doğu İlmak Github</a>

<br>

<h2>Keywords</h2>
<ul>
  <li>Geology</li>
  <li>Earth Science</li>
  <li>Earthquake</li>
  <li>Turkey</li>
  <li>BeautifulSoup</li>
  <li>DBSCAN</li>
</ul>

<br>

<h2>Definition of Earthquake</h2>

<p>An earthquake is the shaking of the surface of the Earth resulting from a sudden release of energy in the Earth's <i>lithosphere</i> that creates <i>seismic waves</i>. People can scale <i>seismic waves</i> as <i>The Richter scale</i>.</p>

<br>

<h2>Definition of the Richter Scale</h2>

<p>The Richter scale —also called the Richter magnitude scale, Richter's magnitude scale, and the <i>Gutenberg–Richter</i> scale—is a measure of the strength of earthquakes, developed by <i>Charles Francis Richter</i> and presented in his landmark <i>1935</i> paper, where he called it the "magnitude scale".This was later revised and renamed the local magnitude scale, denoted as $ML$ or $M_{L}$.</p>

<br>

$$M_{L} = log_{10} A - log_{10} A_{0}(δ) = log_{10} [A/A_{0}(δ)]$$

<br>

<p>$A$ is the maximum excursion of the Wood–Anderson seismograph</p>

<p>The empirical function $A_{0}$ depends only on the epicentral distance of the station, $δ$. In practice, readings from all observing stations are averaged after adjustment with station-specific corrections to obtain the $M_{L}$ value.</p>

<br>

<h2>Kandilli Observatory and Earthquake Research Instıtute (KOERI)</h2>

<p>Kandilli Observatory and Earthquake Research Instıtute (KOERI) determines the location and size of all <i>earthquakes</i> that occur in Turkey and disseminates this information immediately to national and international agencies, scientists, critical facilities, and the general public. <b>This project was created based on data obtained by the Kandilli Observatory and Earthquake Research Institute (KOERI).</b></p>

<h3>Data Link</h3>

You can take a look at original website of <a href='http://www.koeri.boun.edu.tr/sismo/2/tr/'>Kandilli Observatory and Earthquake Research Instıtute (KOERI).</a>

<br>

<h2>License</h2>

<p>MIT License</p>

<br>

<h3>Sources</h3>
<ul>
    <li><a href="https://en.wikipedia.org/wiki/Richter_magnitude_scale">Wikipedia</a></li>
    <li><a href="http://www.koeri.boun.edu.tr/sismo/2/tr/">Kandilli Observatory and Earthquake Research Instıtute (KOERI)</a></li>
</ul>

<br>

<h2>Table of Contents</h2>

<p>The <i>magnitude</i> of the <i>earthquakes</i> has been visualized on the plot and clustered by <i>DBSCAN</i> in Turkey.</p>

<div class="alert alert-block alert-info" style="margin-top: 20px">
<li><a href="https://#import">Import Libraries for Model</a></li>
<li><a href="https://#data_preparation">Dataset Preparation (Data Preprocessing)</a></li>
<li><a href="https://#dbscan">Clustering with DBSCAN</a></li>

<br>

<p>Estimated Time Needed: <strong>20 min</strong></p>

</div>

<br>
<h2 align=center id="import">Import Libraries for Model</h2>
<p>The following are the libraries we are going to use for this lab:</p>
"""

!pip3 install basemap -q

!pip3 install basemap-data-hires -q

from datetime import date
from datetime import timedelta
import datetime

import pandas as pd
import numpy as np

from sklearn.cluster import DBSCAN 
import sklearn.utils
from sklearn.preprocessing import StandardScaler

# Commented out IPython magic to ensure Python compatibility.
from mpl_toolkits.basemap import Basemap

import matplotlib.pyplot as plt
from pylab import rcParams
# %matplotlib inline
rcParams['figure.figsize'] = (20, 20)

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/

# Commented out IPython magic to ensure Python compatibility.
# %ls

"""<br>
<h2 align=center id="data_preparation">Dataset Preparation (Data Preprocessing)</h2>

<p>You should do following steps for the dataset. First, you need to <a href='http://www.koeri.boun.edu.tr/sismo/2/tr/'>Kandilli Observatory and Earthquake Research Instıtute (KOERI)</a> website. After that, you should select month in <code>Aylar</code> section.</p>

<br>

<img src="https://raw.githubusercontent.com/doguilmak/Clustering-Earthquakes-in-Turkey/main/assets/step_1.png" width=1000 height=500 alt="https://github.com/doguilmak/Clustering-Earthquakes-in-Turkey">

<br>

<p>Then, you can see all the earthquakes happened in spesific month with click on the arrow which is on the right bottom. Now, we can download the data. For that, all we need to do is click on save icon.</p>

<br>

<img src="https://raw.githubusercontent.com/doguilmak/Clustering-Earthquakes-in-Turkey/main/assets/step_2.png" width=1000 height=500 alt="https://github.com/doguilmak/Clustering-Earthquakes-in-Turkey">

<br>

<p>We download the dataset as <code>.txt</code> format. We opened the data on Microsoft Excel to modify. In Excel, we made colums for each value. After that, we dropped <i>İlksel</i> values next to the <code>Yer</code> column. Finally, we saved as <code>.csv</code> file.</p>

<br>
"""

path = '/content/20230306115624z.csv'

df = pd.read_csv(path, encoding="utf-8", header=None)
df.columns = ['Oluş zamanı', 'Enlem', 'Boylam', 'Der', 'Mag', 'Yer']

df.head()

df[['Tarih', 'Saat']] = df['Oluş zamanı'].str.split(' ', 1, expand=True)
df.drop(columns=['Oluş zamanı'], inplace=True)

df.head()

df['Tarih'] = pd.to_datetime(df['Tarih'])

df['Saat'] = pd.to_datetime(df['Saat'])
df['Saat'] = [time.time() for time in df['Saat']]

df.head()

df.info()

print("Number of NaN values: {}.".format(df.isnull().sum().sum()))

print("Number of duplicated rows: {}.".format(df.duplicated().sum()))

max(df['Mag'])

"""<h3>Filter Data</h3>"""

df[df['Mag'] > 7]

df[df['Tarih'] == '2023-02-06'].head()

"""<br>
<h2 align=center id="dbscan">Clustering with DBSCAN and Visualization</h2>

<p>The matplotlib basemap toolkit is a library for plotting 2D data on maps in Python. Basemap does not do any plotting on it’s own, but provides the facilities to transform coordinates to a map projections.</p>

<p>Approximate coordinates:</p>
"""

date = datetime.datetime.utcnow()

"""<p>Through our data, we should define edges of the earthquakes.</p>"""

max_lat = df['Enlem'].max()
max_lat = max_lat + 1
min_lat = df['Enlem'].min()
min_lat = min_lat - 1

max_lon = df['Boylam'].max()
max_lon = max_lon + 1
min_lon = df['Boylam'].min()
min_lon = min_lon - 1

my_map = Basemap(projection='merc',
            resolution = 'h', area_thresh = 100.0,
            llcrnrlon=min_lon, llcrnrlat=min_lat, #min longitude (llcrnrlon) and latitude (llcrnrlat)
            urcrnrlon=max_lon, urcrnrlat=max_lat) #max longitude (urcrnrlon) and latitude (urcrnrlat)

my_map.drawcoastlines()
my_map.drawcountries()
my_map.fillcontinents(color = 'white', alpha = 0.3)
my_map.shadedrelief()
my_map.nightshade(date)
parallels = np.arange(round(min_lat), round(max_lat), 1)
my_map.drawparallels(parallels,labels=[True,True,True,True], color='#A9A9A9')
meridians = np.arange(round(min_lon), round(max_lon), 1)
my_map.drawmeridians(meridians,labels=[True,True,True,True], color='#A9A9A9')

xs, ys = my_map(np.asarray(df.Boylam), np.asarray(df.Enlem))
df['xm'] = xs.tolist()
df['ym'] =ys.tolist()

lon = df['Boylam']
lat = df['Enlem']
mag = df['Mag']

my_map.shadedrelief()
my_map.drawcoastlines(color='gray')
my_map.drawcountries(color='gray')
my_map.fillcontinents(color = 'white', alpha = 0.1)
my_map.nightshade(date)
parallels = np.arange(round(min_lat), round(max_lat), 1)
my_map.drawparallels(parallels,labels=[True,True,True,True], color='#A9A9A9')
meridians = np.arange(round(min_lon), round(max_lon), 1)
my_map.drawmeridians(meridians,labels=[True,True,True,True], color='#A9A9A9')

lon = df['Boylam']
lat = df['Enlem']
mag = df['Mag']

nx, ny = 40, 40
lon_bins = np.linspace(min_lon, max_lon, nx+1)
lat_bins = np.linspace(min_lat, max_lat, ny+1)

density, _, _ = np.histogram2d(lon, lat, [lon_bins, lat_bins])

a = my_map.imshow(density.T, interpolation='spline36', alpha=0.7, cmap='YlOrBr', vmin=0, vmax=34)

for index, row in df.iterrows():
   my_map.plot(row.xm, row.ym, markerfacecolor =([0.7, 0, 0]),  marker='o', markersize= 5, alpha = 0.75)
plt.show()

my_map.shadedrelief()
my_map.drawcoastlines(color='gray')
my_map.drawcountries(color='gray')
my_map.fillcontinents(color = 'white', alpha = 0.1)
my_map.nightshade(date)
parallels = np.arange(round(min_lat), round(max_lat), 1)
my_map.drawparallels(parallels,labels=[True,True,True,True], color='#A9A9A9')
meridians = np.arange(round(min_lon), round(max_lon), 1)
my_map.drawmeridians(meridians,labels=[True,True,True,True], color='#A9A9A9')

for index, row in df.iterrows():
   my_map.plot(row.xm, row.ym, markerfacecolor =([1, 0, 0]),  marker='o', markersize= 5, alpha = 0.75)
plt.show()

"""<h3>Clustering of Stations Based on Their Magnitude</h3>

<p><i>DBSCAN</i> form sklearn library can run <i>DBSCAN</i> clustering from vector array or distance matrix. In our case, we pass it the Numpy array Clus_dataSet to find core samples of high density and expands clusters from them.</p>
"""

sklearn.utils.check_random_state(100)
Clus_dataSet = df[['xm', 'ym']]
Clus_dataSet = np.nan_to_num(Clus_dataSet)
Clus_dataSet = StandardScaler().fit_transform(Clus_dataSet)

"""<p>Computing DBSCAN.</p>"""

df.head()

db = DBSCAN(eps=0.15, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
df["Clus_Db"]=labels

labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels))

df[["Mag", "Clus_Db"]].head()

set(labels)

"""<h4>Visualization of Clusters Based on Location</h4>"""

my_map.shadedrelief()
my_map.drawcoastlines(color='gray')
my_map.drawcountries(color='gray')
my_map.fillcontinents(color = 'white', alpha = 0.1)
my_map.nightshade(date)
parallels = np.arange(round(min_lat), round(max_lat), 1)
my_map.drawparallels(parallels,labels=[True,True,True,True], color='#A9A9A9')
meridians = np.arange(round(min_lon), round(max_lon), 1)
my_map.drawmeridians(meridians,labels=[True,True,True,True], color='#A9A9A9')

colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))

for clust_number in set(labels):
    c=(([0.4, 0.4, 0.4]) if clust_number == -1 else colors[np.int(clust_number)])
    clust_set = df[df.Clus_Db == clust_number]                    
    my_map.scatter(clust_set.xm, clust_set.ym, color=c,  marker='o', s=10, alpha = 0.85)
    if clust_number != -1:
        cenx=np.mean(clust_set.xm) 
        ceny=np.mean(clust_set.ym) 
        plt.text(cenx, ceny, str(clust_number), fontsize=18, color='red')
        print ("Cluster " + str(clust_number)+', Avg Magnitude: '+ str(np.mean(clust_set.Mag)))

"""<br>

<h2>Contact Me</h2>
<p>If you have something to say to me please contact me:</p>

<ul>
  <li>Twitter: <a href="https://twitter.com/Doguilmak">Doguilmak</a></li>
  <li>Mail address: doguilmak@gmail.com</li>
</ul>
"""

from datetime import datetime
print(f"Changes have been made to the project on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")